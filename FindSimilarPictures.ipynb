{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Поиск похожих изображений по картинке"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:30.485922Z",
     "start_time": "2024-12-03T13:36:30.479937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights, Inception_V3_Weights\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "IMAGES_FOLDER = \"images_19206\"\n",
    "\n",
    "INCEPTION_OPTION = \"INCEPTION MODEL EMBEDDINGS\"\n",
    "DINO_OPTION = \"DINO MODEL EMBEDDINGS\"\n",
    "VGG_OPTION = \"VGG-16 MODEL EMBEDDINGS\"\n",
    "HOG_OPTION = \"HISTOGRAM OF GRADIENTS EMBEDDINGS\"\n",
    "COLOR_HIST_OPTION = \"COLOR HISTOGRAM EMBEDDINGS\"\n",
    "SIFT_OPTION = \"SIFT EMBEDDINGS\""
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:32.078399Z",
     "start_time": "2024-12-03T13:36:30.557974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dino_processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n",
    "dino_model = AutoModel.from_pretrained('facebook/dinov2-small').to(device)"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:34.667610Z",
     "start_time": "2024-12-03T13:36:32.131435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vgg_model = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
    "vgg_model.classifier = vgg_model.classifier[0]\n",
    "vgg_model = vgg_model.to(device)"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:35.461605Z",
     "start_time": "2024-12-03T13:36:34.705150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "inception_model = models.inception_v3(weights=Inception_V3_Weights.DEFAULT)\n",
    "inception_model.fc = torch.nn.Identity()\n",
    "inception_model = inception_model.to(device)"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:35.518922Z",
     "start_time": "2024-12-03T13:36:35.494921Z"
    }
   },
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def calculate_embeddings(calculate_embedding_method, output_file_name, images_path=IMAGES_FOLDER):\n",
    "    df = pd.DataFrame(data=None, columns=[\"img_path\", \"vector\"])\n",
    "    df_index = 0\n",
    "    \n",
    "    bar = tqdm.tqdm(total=len(os.listdir(images_path)))\n",
    "    for img in os.listdir(images_path):\n",
    "        img_path = os.path.join(images_path, img)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        img_vector = calculate_embedding_method(img)\n",
    "        \n",
    "        df.loc[df_index] = [img_path, img_vector]\n",
    "        df_index += 1\n",
    "        bar.update(1)\n",
    "        \n",
    "    df.to_pickle(output_file_name)\n",
    "\n",
    "        \n",
    "def sift_descriptors(img):\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    gray_img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    sift = cv2.SIFT_create()\n",
    "    _, img_descriptors = sift.detectAndCompute(gray_img, None)\n",
    "    return img_descriptors\n",
    "\n",
    "\n",
    "def color_histogram(img):\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    result = []\n",
    "    colors = (\"red\", \"green\", \"blue\")\n",
    "    for channel_id, color in enumerate(colors):\n",
    "        histogram, _ = np.histogram(img[:, :, channel_id], bins=256)\n",
    "        histogram = histogram / np.linalg.norm(histogram)\n",
    "        result.extend(histogram)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def hog(img):\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    g, theta = cv2.cartToPolar(sobel_x, sobel_y)\n",
    "    hist, _ = np.histogram(theta.flatten(), bins=256, range=(0, 2*np.pi), weights=g.flatten())\n",
    "    hist = hist / np.linalg.norm(hist)\n",
    "    \n",
    "    return hist\n",
    "\n",
    "\n",
    "def dino_embedding(img):\n",
    "    img = cv2.resize(img, (512, 512))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = dino_processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        outputs = dino_model(**inputs)\n",
    "        \n",
    "    features = outputs.last_hidden_state\n",
    "    embedding = features.mean(dim=1).squeeze().cpu().detach().numpy()\n",
    "    embedding = np.float32(embedding) / np.linalg.norm(embedding)\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def vgg_16_embedding(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(214),\n",
    "        transforms.CenterCrop(214),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(img)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vgg_model.eval()\n",
    "        embedding = vgg_model(input_tensor).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    embedding = np.float32(embedding) / np.linalg.norm(embedding)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def inception_embedding(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    input_tensor = preprocess(img)\n",
    "    input_tensor = input_tensor.unsqueeze(0)\n",
    "    input_tensor = input_tensor.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inception_model.eval()\n",
    "        embedding = inception_model(input_tensor).squeeze().cpu().detach().numpy()\n",
    "\n",
    "    embedding = np.float32(embedding) / np.linalg.norm(embedding)\n",
    "    return embedding"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T13:36:35.558900Z",
     "start_time": "2024-12-03T13:36:35.552900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RUN ONLY 1 TIME:\n",
    "if not os.path.isdir(f\"{IMAGES_FOLDER}_vectors\"):\n",
    "    os.makedirs(f\"{IMAGES_FOLDER}_vectors\")\n",
    "\n",
    "# Classic computer vision methods:\n",
    "# calculate_embeddings(sift_descriptors, f\"{IMAGES_FOLDER}_vectors/sift_vectors.pkl\")\n",
    "# calculate_embeddings(color_histogram, f\"{IMAGES_FOLDER}_vectors/color_histogram_vectors.pkl\")\n",
    "# calculate_embeddings(hog, f\"{IMAGES_FOLDER}_vectors/hog_vectors.pkl\")\n",
    "\n",
    "# Deep learning methods:\n",
    "# calculate_embeddings(dino_embedding, f\"{IMAGES_FOLDER}_vectors/dino_vectors.pkl\")\n",
    "# calculate_embeddings(vgg_16_embedding, f\"{IMAGES_FOLDER}_vectors/vgg_16_vectors.pkl\")\n",
    "# calculate_embeddings(inception_embedding, f\"{IMAGES_FOLDER}_vectors/inception_vectors.pkl\")"
   ],
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:45.478947Z",
     "start_time": "2024-12-03T14:11:45.465477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_similar_images(image, method = SIFT_OPTION, top_k = 30):\n",
    "    if method == SIFT_OPTION:\n",
    "        images_embeddings = pd.read_pickle(f\"{IMAGES_FOLDER}_vectors/sift_vectors.pkl\")\n",
    "        query_embedding = sift_descriptors(image)\n",
    "    elif method == COLOR_HIST_OPTION:\n",
    "        images_embeddings = pd.read_pickle(f\"{IMAGES_FOLDER}_vectors/color_histogram_vectors.pkl\")\n",
    "        query_embedding = color_histogram(image)\n",
    "    elif method == HOG_OPTION:\n",
    "        images_embeddings = pd.read_pickle(f\"{IMAGES_FOLDER}_vectors/hog_vectors.pkl\")\n",
    "        query_embedding = hog(image)\n",
    "    elif method == DINO_OPTION:\n",
    "        images_embeddings = pd.read_pickle(f\"{IMAGES_FOLDER}_vectors/dino_vectors.pkl\")\n",
    "        query_embedding = dino_embedding(image)\n",
    "    elif method == VGG_OPTION:\n",
    "        images_embeddings = pd.read_pickle(f\"{IMAGES_FOLDER}_vectors/vgg_16_vectors.pkl\")\n",
    "        query_embedding = vgg_16_embedding(image)\n",
    "    elif method == INCEPTION_OPTION:\n",
    "        images_embeddings = pd.read_pickle(f\"{IMAGES_FOLDER}_vectors/inception_vectors.pkl\")\n",
    "        query_embedding = inception_embedding(image)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "        \n",
    "    all_img_paths = images_embeddings[\"img_path\"]\n",
    "    all_embeddings = images_embeddings[\"vector\"]\n",
    "    \n",
    "    if method == SIFT_OPTION:\n",
    "        all_num_matches = []\n",
    "        \n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "        for i, vector in enumerate(all_embeddings):\n",
    "            matches = bf.match(query_embedding, vector)\n",
    "            num_matches = len(matches)\n",
    "            all_num_matches.append(num_matches)\n",
    "            \n",
    "        sorted_results = sorted(zip(all_num_matches, all_img_paths), key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "    else:\n",
    "        all_distances = []\n",
    "        \n",
    "        for i, vector in enumerate(all_embeddings):\n",
    "            if method == DINO_OPTION or method == VGG_OPTION or method == INCEPTION_OPTION:\n",
    "                cosine_similarity = np.dot(query_embedding, vector)\n",
    "                distance = 1 - cosine_similarity\n",
    "                if (method == DINO_OPTION and distance < 0.03) or (method != DINO_OPTION and distance < 0.1):\n",
    "                    distance = 1\n",
    "            else:\n",
    "                distance = np.linalg.norm(query_embedding - vector)\n",
    "            \n",
    "            all_distances.append(distance)\n",
    "        \n",
    "        sorted_results = sorted(zip(all_distances, all_img_paths), key=lambda x: x[0])\n",
    "\n",
    "    result_images = []\n",
    "    for i in range(top_k):\n",
    "        img_path = sorted_results[i][1]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        result_images.append(img)\n",
    "    return result_images"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T15:26:17.117552Z",
     "start_time": "2024-12-03T15:26:16.018755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def get_similar_images(image, slider_value, method):\n",
    "    similar_images = find_similar_images(image, method, slider_value)\n",
    "    images = []\n",
    "    \n",
    "    for image in similar_images:\n",
    "        images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB),)\n",
    "    \n",
    "    return similar_images\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    return f\"Processed text: {text}\"\n",
    "\n",
    "\n",
    "with gr.Blocks() as gradio_interface:\n",
    "    choice = gr.Radio([\"Image Input\", \"Text Input\"], label=\"Choose Input Type\", value=\"Image Input\")\n",
    "\n",
    "    with gr.Group(visible=True) as image_block:\n",
    "        with gr.Row(variant=\"compact\"):\n",
    "            with gr.Column(variant=\"compact\"):\n",
    "                image_input = gr.Image(label=\"Upload Image\", height=300)\n",
    "                top_k_slider = gr.Slider(1, 300, 30, step=1, label=\"Similar Images Number\")\n",
    "                option_dropdown = gr.Dropdown(\n",
    "                    choices=[DINO_OPTION, VGG_OPTION, INCEPTION_OPTION, HOG_OPTION, COLOR_HIST_OPTION, SIFT_OPTION],\n",
    "                    label=\"Choose Embedding Method\"\n",
    "                )\n",
    "                process_image_button = gr.Button(\"Find Similar Images\")\n",
    "\n",
    "            with gr.Column(variant=\"compact\"):\n",
    "                image_output = gr.Gallery(label=\"Similar Images\", columns=3)\n",
    "\n",
    "            process_image_button.click(\n",
    "                get_similar_images,\n",
    "                inputs=[image_input, top_k_slider, option_dropdown],\n",
    "                outputs=[image_output]\n",
    "            )\n",
    "\n",
    "    with gr.Group(visible=False) as text_block:\n",
    "        with gr.Row(variant=\"compact\"):\n",
    "            with gr.Column(variant=\"compact\"):\n",
    "                text_input = gr.Textbox(label=\"Enter Text\")\n",
    "                process_text_button = gr.Button(\"Submit Text\")\n",
    "            \n",
    "            with gr.Column(variant=\"compact\"):\n",
    "                image_output = gr.Gallery(label=\"Similar Images\", columns=3)\n",
    "\n",
    "        process_text_button.click(\n",
    "            process_text,\n",
    "            inputs=[text_input],\n",
    "            outputs=[image_output]\n",
    "        )\n",
    "\n",
    "    def update_interface(choice):\n",
    "        if choice == \"Text Input\":\n",
    "            return gr.update(visible=False), gr.update(visible=True)\n",
    "        elif choice == \"Image Input\":\n",
    "            return gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "    choice.change(\n",
    "        update_interface,\n",
    "        inputs=[choice],\n",
    "        outputs=[image_block, text_block]\n",
    "    )\n",
    "\n",
    "gradio_interface.launch()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7899\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7899/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 166
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
